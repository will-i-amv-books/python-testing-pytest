{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Test Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%pip install -e ../tasks_proj/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's install the 'tasks' package locally, using the -e flag \n",
    "# so we can modify the source code while 'tasks' is installed.\n",
    "\n",
    "\"\"\"\n",
    "%pip install -e ../tasks_proj/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '../tasks_proj'\n",
    "TESTS_PATH = f'{ROOT_PATH}/tests/'\n",
    "UNIT_TESTS_PATH = f'{TESTS_PATH}/unit/'\n",
    "FUNC_TESTS_PATH = f'{TESTS_PATH}/func/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/unit/test_task.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                               [ 66%]\u001b[0m\n",
      "../tasks_proj/tests/unit/test_task_fail.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_task_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_task_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33msit there\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbrian\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        t2 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mdo something\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m t1 == t2\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert Task(summary=...alse, id=None) == Task(summary=...alse, id=None)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 2 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing attributes:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ['summary', 'owner']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Drill down into differing attribute summary:\u001b[0m\n",
      "\u001b[1m\u001b[31mE           summary: 'sit there' != 'do something'...\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         ...Full output truncated (9 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:9: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_dict_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dict_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks compared as dicts should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1_dict = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mmake sandwich\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)._asdict()\n",
      "        t2_dict = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mmake sandwich\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokkem\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)._asdict()\n",
      ">       \u001b[94massert\u001b[39;49;00m t1_dict == t2_dict\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert {'done': Fals...ake sandwich'} == {'done': Fals...ake sandwich'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 3 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'owner': 'okken'} != {'owner': 'okkem'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:16: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_task_equality - Asser...\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_dict_equality - Asser...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.37s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Then we can run the unit tests as follows\n",
    "\n",
    "!pytest {UNIT_TESTS_PATH}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using assert Statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/unit/test_task_fail.py \u001b[31mF\u001b[0m\u001b[31m                             [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_task_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_task_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33msit there\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbrian\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        t2 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mdo something\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m t1 == t2\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert Task(summary=...alse, id=None) == Task(summary=...alse, id=None)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 2 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing attributes:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ['summary', 'owner']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Drill down into differing attribute summary:\u001b[0m\n",
      "\u001b[1m\u001b[31mE           summary: 'sit there' != 'do something'...\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         ...Full output truncated (9 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:9: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_task_equality - Asser...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.17s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Pytest intercepts assert calls and replaces them with additional info\n",
    "# that shows more about why the assertions failed\n",
    "\n",
    "!pytest {UNIT_TESTS_PATH}/test_task_fail.py::test_task_equality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 7 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can force exceptions in test functions, then use pytest to assert that \n",
    "# the exception happened:\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_api_exceptions.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking Test Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 50 items / 47 deselected / 3 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add.py \u001b[32m.\u001b[0m\u001b[32m                                   [ 33%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m3 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Tests can have more than one marker, and a marker can be on multiple tests.\n",
    "\n",
    "!pytest -m 'smoke' {FUNC_TESTS_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 50 items / 49 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m49 deselected\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run tests marked as 'get' or 'smoke' from all test files, as follows\n",
    "\n",
    "!pytest -m 'get' {FUNC_TESTS_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 50 items / 47 deselected / 3 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add.py \u001b[32m.\u001b[0m\u001b[32m                                   [ 33%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m3 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -m 'smoke' {FUNC_TESTS_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 50 items / 48 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add.py \u001b[32m.\u001b[0m\u001b[32m                                   [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m48 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also combine markers\n",
    "\n",
    "!pytest -m 'smoke and not get' {FUNC_TESTS_PATH}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_unique_id_2.py::test_unique_id_1 \u001b[33mSKIPPED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_2.py::test_unique_id_2 \u001b[32mPASSED\u001b[0m\u001b[32m    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can skip tests using the 'skip' marker, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_unique_id_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_unique_id_3.py::test_unique_id_1 \u001b[33mSKIPPED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_3.py::test_unique_id_2 \u001b[32mPASSED\u001b[0m\u001b[32m    [100%]\u001b[0m\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "SKIPPED [1] ../tasks_proj/tests/func/test_unique_id_3.py:7: not supported until version 0.2.0\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also skip tests that don't pass a certain condition \n",
    "# with the 'skipif' marker, as follows\n",
    "\n",
    "!pytest -rs -v {FUNC_TESTS_PATH}/test_unique_id_3.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking Tests as Expecting to Fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_1 \u001b[33mXFAIL\u001b[0m\u001b[32m     [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_is_a_duck \u001b[33mXFAIL\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_not_a_duck \u001b[33mXPASS\u001b[0m\u001b[33m [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_2 \u001b[32mPASSED\u001b[0m\u001b[33m    [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m2 xfailed\u001b[0m, \u001b[33m\u001b[1m1 xpassed\u001b[0m\u001b[33m in 0.17s\u001b[0m\u001b[33m ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can mark some tests that we expect them to fail\n",
    "# with the 'xfail' marker, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_unique_id_4.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Subset of Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run all test from a single class, as follows\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_api_exceptions.py::TestUpdate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run a single method from a class, as follows\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_api_exceptions.py::TestUpdate::test_bad_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 56 items / 51 deselected / 5 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_add_raises \u001b[32mPASSED\u001b[0m\u001b[32m  [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_start_tasks_db_raises \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_list_raises \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_get_raises \u001b[32mPASSED\u001b[0m\u001b[32m  [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_delete_raises \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m5 passed\u001b[0m, \u001b[33m51 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run test functions that have '_raises' in their name, as follows\n",
    "\n",
    "!pytest -v {TESTS_PATH} -k \"_raises\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 56 items / 52 deselected / 4 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_add_raises \u001b[32mPASSED\u001b[0m\u001b[32m  [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_start_tasks_db_raises \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_list_raises \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_get_raises \u001b[32mPASSED\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m52 deselected\u001b[0m\u001b[32m in 0.19s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can specify more complex conditions, as follows\n",
    "\n",
    "!pytest -v {TESTS_PATH} -k \"_raises and not delete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrized Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py \u001b[32m.\u001b[0m\u001b[32m                           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The following test is with a single 'Task' example\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_add_variety.py::test_add_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task2] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task3] \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can test the last function with more than one 'Task' objects\n",
    "# with the 'pytest.mark.parametrize' decorator\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[sleep-None-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[wake-brian-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[breathe-BRIAN-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[eat eggs-BrIaN-False] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can pass a list of attributes for the 'Task' object\n",
    "# instead of a list of 'Task' objects, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[sleep-None-False] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can specify one set of attributes for the 'Task' object, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_3[sleep-None-False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task2] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task3] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task4] \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.13s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can use a list of 'Task' objects stored in another variable\n",
    "# as parameters for the 'parametrize' decorator, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(sleep,None,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(wake,brian,False)0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(wake,brian,False)1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(breathe,BRIAN,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(exercise,BrIaN,False)] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.13s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can set our own parameter IDs to improve the readability \n",
    "# of the test output, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 10 items                                                             \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(sleep,None,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(wake,brian,False)0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(wake,brian,False)1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(breathe,BRIAN,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(exercise,BrIaN,False)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(sleep,None,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(wake,brian,False)0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(wake,brian,False)1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(breathe,BRIAN,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(exercise,BrIaN,False)] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also parametrize tests classes, as follows:\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::TestAdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_6[just summary] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_6[summary/owner] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_6[summary/owner/done] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also identify individual parameters in a list of examples \n",
    "# by including a meaningful description with the 'pytest.param' function\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "597ed2a883dd92248704c39695e4adb59d169bf662e25147d30ca9618a6f5c2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
