{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Test Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install -e ../tasks_proj/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's install the 'tasks' package locally, using the -e flag \n",
    "# so we can modify the source code while 'tasks' is installed.\n",
    "\n",
    "\"\"\"\n",
    "!pip install -e ../tasks_proj/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '../tasks_proj'\n",
    "UNIT_TESTS_PATH = '../tasks_proj/tests/unit'\n",
    "FUNC_TESTS_PATH = '../tasks_proj/tests/func'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/unit/test_task.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                               [ 66%]\u001b[0m\n",
      "../tasks_proj/tests/unit/test_task_fail.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_task_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_task_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33msit there\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbrian\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        t2 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mdo something\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m t1 == t2\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert Task(summary=...alse, id=None) == Task(summary=...alse, id=None)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 2 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing attributes:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ['summary', 'owner']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Drill down into differing attribute summary:\u001b[0m\n",
      "\u001b[1m\u001b[31mE           summary: 'sit there' != 'do something'...\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         ...Full output truncated (9 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:9: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_dict_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dict_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks compared as dicts should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1_dict = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mmake sandwich\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)._asdict()\n",
      "        t2_dict = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mmake sandwich\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokkem\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)._asdict()\n",
      ">       \u001b[94massert\u001b[39;49;00m t1_dict == t2_dict\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert {'done': Fals...ake sandwich'} == {'done': Fals...ake sandwich'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 3 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'owner': 'okken'} != {'owner': 'okkem'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:16: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_task_equality - Asser...\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_dict_equality - Asser...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Then we can run the unit tests as follows\n",
    "\n",
    "!pytest {UNIT_TESTS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using assert Statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/unit/test_task_fail.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_task_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_task_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33msit there\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbrian\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        t2 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mdo something\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m t1 == t2\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert Task(summary=...alse, id=None) == Task(summary=...alse, id=None)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 2 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing attributes:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ['summary', 'owner']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Drill down into differing attribute summary:\u001b[0m\n",
      "\u001b[1m\u001b[31mE           summary: 'sit there' != 'do something'...\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         ...Full output truncated (9 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:9: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_dict_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dict_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks compared as dicts should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1_dict = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mmake sandwich\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)._asdict()\n",
      "        t2_dict = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mmake sandwich\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokkem\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)._asdict()\n",
      ">       \u001b[94massert\u001b[39;49;00m t1_dict == t2_dict\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert {'done': Fals...ake sandwich'} == {'done': Fals...ake sandwich'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 3 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'owner': 'okken'} != {'owner': 'okkem'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:16: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_task_equality - Asser...\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_dict_equality - Asser...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m2 failed\u001b[0m\u001b[31m in 0.08s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can test only the failing tests, as follows\n",
    "\n",
    "!pytest {UNIT_TESTS_PATH}/test_task_fail.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/unit/test_task_fail.py \u001b[31mF\u001b[0m\u001b[31m                             [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_task_equality ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_task_equality\u001b[39;49;00m():\n",
      "        \u001b[33m\"\"\"Different tasks should not be equal.\"\"\"\u001b[39;49;00m\n",
      "        t1 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33msit there\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbrian\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        t2 = Task(\u001b[33m'\u001b[39;49;00m\u001b[33mdo something\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mokken\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m t1 == t2\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert Task(summary=...alse, id=None) == Task(summary=...alse, id=None)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 2 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing attributes:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ['summary', 'owner']\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         Drill down into differing attribute summary:\u001b[0m\n",
      "\u001b[1m\u001b[31mE           summary: 'sit there' != 'do something'...\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         ...Full output truncated (9 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../tasks_proj/tests/unit/test_task_fail.py\u001b[0m:9: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED ../tasks_proj/tests/unit/test_task_fail.py::test_task_equality - Asser...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can test only one failing test, as follows\n",
    "\n",
    "!pytest {UNIT_TESTS_PATH}/test_task_fail.py::test_task_equality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 7 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m7 passed\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also run the functional tests, as follows\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_api_exceptions.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking Test Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 7 items / 5 deselected / 2 selected                                  \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m5 deselected\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run tests marked as 'smoke' and 'get', as follows\n",
    "\n",
    "!pytest -m 'smoke' {FUNC_TESTS_PATH}/test_api_exceptions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 7 items / 6 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[33m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m6 deselected\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -m 'get' {FUNC_TESTS_PATH}/test_api_exceptions.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj\n",
      "collected 24 items / 21 deselected / 3 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add.py \u001b[32m.\u001b[0m\u001b[33m                                   [ 33%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_add.py:17\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_add.py:17: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m21 deselected\u001b[0m, \u001b[33m\u001b[1m4 warnings\u001b[0m\u001b[33m in 0.05s\u001b[0m\u001b[33m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run tests marked as 'smoke' from all test files, as follows\n",
    "\n",
    "!pytest -m 'smoke' {ROOT_PATH}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_unique_id_2.py::test_unique_id_1 \u001b[33mSKIPPED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_2.py::test_unique_id_2 \u001b[32mPASSED\u001b[0m\u001b[32m    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also skip some tests, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_unique_id_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_unique_id_3.py::test_unique_id_1 \u001b[33mSKIPPED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_3.py::test_unique_id_2 \u001b[32mPASSED\u001b[0m\u001b[32m    [100%]\u001b[0m\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "SKIPPED [1] ../tasks_proj/tests/func/test_unique_id_3.py:7: not supported until version 0.2.0\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also skip some tests conditionally, as follows\n",
    "\n",
    "!pytest -rs -v {FUNC_TESTS_PATH}/test_unique_id_3.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking Tests as Expecting to Fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_1 \u001b[33mXFAIL\u001b[0m\u001b[32m     [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_is_a_duck \u001b[33mXFAIL\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_not_a_duck \u001b[33mXPASS\u001b[0m\u001b[33m [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_unique_id_4.py::test_unique_id_2 \u001b[32mPASSED\u001b[0m\u001b[33m    [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m2 xfailed\u001b[0m, \u001b[33m\u001b[1m1 xpassed\u001b[0m\u001b[33m in 0.05s\u001b[0m\u001b[33m ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also mark some tests that we expect them to fail, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_unique_id_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Subset of Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run all test from a single class in a module, as follows\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_api_exceptions.py::TestUpdate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py \u001b[32m.\u001b[0m\u001b[33m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can even run a single method from single class in a module, as follows\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_api_exceptions.py::TestUpdate::test_bad_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj\n",
      "collected 24 items / 19 deselected / 5 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_add_raises \u001b[32mPASSED\u001b[0m\u001b[33m  [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_start_tasks_db_raises \u001b[32mPASSED\u001b[0m\u001b[33m [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_list_raises \u001b[32mPASSED\u001b[0m\u001b[33m [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_get_raises \u001b[32mPASSED\u001b[0m\u001b[33m  [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_delete_raises \u001b[32mPASSED\u001b[0m\u001b[33m [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_add.py:17\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_add.py:17: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m5 passed\u001b[0m, \u001b[33m\u001b[1m19 deselected\u001b[0m, \u001b[33m\u001b[1m4 warnings\u001b[0m\u001b[33m in 0.03s\u001b[0m\u001b[33m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can run all of the test functions that have '_raises' in their name, as follows\n",
    "\n",
    "!pytest -v {ROOT_PATH} -k \"_raises\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj\n",
      "collected 24 items / 20 deselected / 4 selected                                \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_add_raises \u001b[32mPASSED\u001b[0m\u001b[33m  [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_start_tasks_db_raises \u001b[32mPASSED\u001b[0m\u001b[33m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_list_raises \u001b[32mPASSED\u001b[0m\u001b[33m [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_api_exceptions.py::test_get_raises \u001b[32mPASSED\u001b[0m\u001b[33m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../tasks_proj/tests/func/test_add.py:17\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_add.py:17: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:20\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:20: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:27\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.get - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.get\n",
      "\n",
      "../tasks_proj/tests/func/test_api_exceptions.py:28\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests/func/test_api_exceptions.py:28: PytestUnknownMarkWarning: Unknown pytest.mark.smoke - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
      "    @pytest.mark.smoke\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m4 passed\u001b[0m, \u001b[33m\u001b[1m20 deselected\u001b[0m, \u001b[33m\u001b[1m4 warnings\u001b[0m\u001b[33m in 0.04s\u001b[0m\u001b[33m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can specify more complex conditions, as follows\n",
    "\n",
    "!pytest -v {ROOT_PATH} -k \"_raises and not delete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrized Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py \u001b[32m.\u001b[0m\u001b[32m                           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Consider the following test\n",
    "\n",
    "!pytest {FUNC_TESTS_PATH}/test_add_variety.py::test_add_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task2] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_2[task3] \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can specify a parameter name and parameter values to the test function, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[sleep-None-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[wake-brian-False] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[breathe-BRIAN-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[eat eggs-BrIaN-False] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can pass multiple parameters to the test function, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_3[sleep-None-False] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can specify one parameter value, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_3[sleep-None-False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task2] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task3] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_4[task4] \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can use variables outside of the funcion as parameters, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(sleep,None,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(wake,brian,False)0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(wake,brian,False)1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(breathe,BRIAN,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_5[Task(exercise,BrIaN,False)] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can use parameter IDs to improve the test output, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 10 items                                                             \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(sleep,None,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(wake,brian,False)0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(wake,brian,False)1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(breathe,BRIAN,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_equivalent[Task(exercise,BrIaN,False)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(sleep,None,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(wake,brian,False)0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(wake,brian,False)1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(breathe,BRIAN,True)] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::TestAdd::test_valid_id[Task(exercise,BrIaN,False)] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 0.16s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also parametrize tests classes, as follows:\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::TestAdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/tasks_proj/tests, configfile: pytest.ini\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_6[just summary] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_6[summary/owner] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "../tasks_proj/tests/func/test_add_variety.py::test_add_6[summary/owner/done] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can also identify parameters by including an id right alongside the parameter value \n",
    "# when passing in a list within the @pytest.mark.parametrize() decorator, as follows\n",
    "\n",
    "!pytest -v {FUNC_TESTS_PATH}/test_add_variety.py::test_add_6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "597ed2a883dd92248704c39695e4adb59d169bf662e25147d30ca9618a6f5c2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
