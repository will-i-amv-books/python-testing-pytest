{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_1 = './example_1'\n",
    "EXAMPLE_2 = './example_2'\n",
    "EXAMPLE_3 = './example_3'\n",
    "EXAMPLE_4 = './example_4'\n",
    "EXAMPLE_5 = './example_5'\n",
    "EXAMPLE_6 = './example_6'\n",
    "EXAMPLE_7 = './example_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../tasks_proj'\n",
    "TESTS = f'{ROOT}/tests'\n",
    "UNIT_TESTS = f'{TESTS}/unit'\n",
    "FUNC_TESTS = f'{TESTS}/func'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/chapter04\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "example_1/test_tmpdir.py \u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The 'tmpdir' fixture is used for single test functions\n",
    "\n",
    "!pytest -s {EXAMPLE_1}/test_tmpdir.py::test_tmpdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/chapter04\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "example_1/test_tmpdir.py \n",
      "base: /tmp/pytest-of-william/pytest-119\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The 'tmpdir_factory' fixture is used for a whole test session\n",
    "\n",
    "!pytest -s {EXAMPLE_1}/test_tmpdir.py::test_tmpdir_factory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/chapter04\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "example_2/test_authors.py file: /tmp/pytest-of-william/pytest-120/data0/author_file.json\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The 'tmpdir_factory' fixture is used for other scopes, such as a module scope\n",
    "\n",
    "!pytest -s {EXAMPLE_2}/test_authors.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"foo\" set to: bar\n",
      "\"myopt\" set to: False\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The hook function 'pytest_addoption' allows us to add addition command line options to pytest.\n",
    "# We can then access the created (and the built-in) options with the 'pytestconfig' fixture\n",
    "\n",
    "!pytest -s -q {EXAMPLE_3}/test_config.py::test_option \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"foo\" set to: bar\n",
      "\"myopt\" set to: True\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can set one or more of the options like any pytest built-in option\n",
    "\n",
    "!pytest -s -q --myopt {EXAMPLE_3}/test_config.py::test_option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"foo\" set to: baz\n",
      "\"myopt\" set to: True\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -s -q --myopt --foo baz {EXAMPLE_3}/test_config.py::test_option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                    [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ test_a[1e+25-1e+23-1.1e+25] __________________________\u001b[0m\n",
      "\n",
      "x = 1e+25, y = 1e+23, expected = 1.1e+25\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mx,y,expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, testdata)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_a\u001b[39;49;00m(x, y, expected):\n",
      "        \u001b[33m\"\"\"Demo approx().\"\"\"\u001b[39;49;00m\n",
      "        sum_ = x + y\n",
      ">       \u001b[94massert\u001b[39;49;00m sum_ == approx(expected)\n",
      "\u001b[1m\u001b[31mE       assert 1.01e+25 == 1.1e+25 ± 1.1e+19\u001b[0m\n",
      "\u001b[1m\u001b[31mE         comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Obtained: 1.01e+25\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Expected: 1.1e+25 ± 1.1e+19\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexample_4/test_few_failures.py\u001b[0m:20: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED example_4/test_few_failures.py::test_a[1e+25-1e+23-1.1e+25] - assert 1...\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Consider the following test with 1 failure\n",
    "\n",
    "!pytest -q {EXAMPLE_4}/test_few_failures.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                        [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ test_a[1e+25-1e+23-1.1e+25] __________________________\u001b[0m\n",
      "\n",
      "x = 1e+25, y = 1e+23, expected = 1.1e+25\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mx,y,expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, testdata)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_a\u001b[39;49;00m(x, y, expected):\n",
      "        \u001b[33m\"\"\"Demo approx().\"\"\"\u001b[39;49;00m\n",
      "        sum_ = x + y\n",
      ">       \u001b[94massert\u001b[39;49;00m sum_ == approx(expected)\n",
      "\u001b[1m\u001b[31mE       assert 1.01e+25 == 1.1e+25 ± 1.1e+19\u001b[0m\n",
      "\u001b[1m\u001b[31mE         comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Obtained: 1.01e+25\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Expected: 1.1e+25 ± 1.1e+19\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mexample_4/test_few_failures.py\u001b[0m:20: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED example_4/test_few_failures.py::test_a[1e+25-1e+23-1.1e+25] - assert 1...\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# I  this case, we can specity the parameter name to run only the failing case\n",
    "\n",
    "!pytest -q {EXAMPLE_4}/test_few_failures.py::test_a[1e+25-1e+23-1.1e+25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                        [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ test_a[1e+25-1e+23-1.1e+25] __________________________\u001b[0m\n",
      "\n",
      "x = 1e+25, y = 1e+23, expected = 1.1e+25\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mx,y,expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, testdata)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_a\u001b[39;49;00m(x, y, expected):\n",
      "        \u001b[33m\"\"\"Demo approx().\"\"\"\u001b[39;49;00m\n",
      "        sum_ = x + y\n",
      ">       \u001b[94massert\u001b[39;49;00m sum_ == approx(expected)\n",
      "\u001b[1m\u001b[31mE       assert 1.01e+25 == 1.1e+25 ± 1.1e+19\u001b[0m\n",
      "\u001b[1m\u001b[31mE         comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Obtained: 1.01e+25\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Expected: 1.1e+25 ± 1.1e+19\u001b[0m\n",
      "\n",
      "expected   = 1.1e+25\n",
      "sum_       = 1.01e+25\n",
      "x          = 1e+25\n",
      "y          = 1e+23\n",
      "\n",
      "\u001b[1m\u001b[31mexample_4/test_few_failures.py\u001b[0m:20: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED example_4/test_few_failures.py::test_a[1e+25-1e+23-1.1e+25] - assert 1...\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# But in cases we can't indentify easily what's wrong. In those cases, \n",
    "# running the failing tests and showing the local variables can help\n",
    "\n",
    "!pytest -q --lf -l {EXAMPLE_4}/test_few_failures.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/chapter04\n",
      "cachedir: /home/william/github/will-i-amv-books/python-testing-pytest/chapter04/.pytest_cache\n",
      "----------------------------- cache values for '*' -----------------------------\n",
      "cache/lastfailed contains:\n",
      "  {'example_4/test_few_failures.py::test_a[1e+25-1e+23-1.1e+25]': True,\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[1]': True,\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[3]': True}\n",
      "cache/nodeids contains:\n",
      "  ['example_1/test_tmpdir.py::test_tmpdir',\n",
      "   'example_1/test_tmpdir.py::test_tmpdir_factory',\n",
      "   'example_1/test_warnings.py::test_lame_function',\n",
      "   'example_1/test_warnings.py::test_lame_function_2',\n",
      "   'example_2/test_authors.py::test_all_have_cities',\n",
      "   'example_2/test_authors.py::test_brian_in_portland',\n",
      "   'example_3/test_config.py::test_option',\n",
      "   'example_4/test_few_failures.py::test_a[0.1-0.2-0.3]',\n",
      "   'example_4/test_few_failures.py::test_a[1.01-2.01-3.02]',\n",
      "   'example_4/test_few_failures.py::test_a[1.23-3.21-4.44]',\n",
      "   'example_4/test_few_failures.py::test_a[1e+25-1e+23-1.1e+25]',\n",
      "   'example_4/test_few_failures.py::test_a[1e+25-1e+24-1.1e+25]',\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[0]',\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[1]',\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[2]',\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[3]',\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[4]',\n",
      "   'example_5/test_capsys.py::test_capsys_disabled',\n",
      "   'example_5/test_capsys.py::test_greeting',\n",
      "   'example_5/test_capsys.py::test_yikes',\n",
      "   'example_6/test_cheese.py::test_def_prefs_change_defaults',\n",
      "   'example_6/test_cheese.py::test_def_prefs_change_expanduser',\n",
      "   'example_6/test_cheese.py::test_def_prefs_change_home',\n",
      "   'example_6/test_cheese.py::test_def_prefs_full',\n",
      "   'example_6/test_monkey.py::test_env',\n",
      "   'example_6/test_monkey.py::test_point_missing_x',\n",
      "   'example_6/test_monkey.py::test_point_repr',\n",
      "   'example_6/test_monkey.py::test_point_str',\n",
      "   'example_6/test_monkey.py::test_prepend',\n",
      "   'example_7/unnecessary_math.py::unnecessary_math',\n",
      "   'example_7/unnecessary_math.py::unnecessary_math.divide',\n",
      "   'example_7/unnecessary_math.py::unnecessary_math.multiply']\n",
      "cache/stepwise contains:\n",
      "  []\n",
      "duration/testdurations contains:\n",
      "  {'example_4/test_slower_2.py::test_slow_stuff[0]': 0.397798,\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[1]': 0.959717,\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[2]': 0.196445,\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[3]': 0.709053,\n",
      "   'example_4/test_slower_2.py::test_slow_stuff[4]': 0.674366}\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.00s\u001b[0m\u001b[33m =============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The last examples work because pytest stores test information from the last session.\n",
    "# We can see them as follows\n",
    "\n",
    "!pytest --cache-show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 2.70s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# To use the cache in test code, we can use the 'cache' fixture.\n",
    "# For example, we can measure how a test execution time compares to last execution times:\n",
    "\n",
    "!pytest -q --cache-clear {EXAMPLE_4}/test_slower.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mE\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                   [100%]\u001b[0m\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m___________________ ERROR at teardown of test_slow_stuff[3] ____________________\u001b[0m\n",
      "\u001b[1m\u001b[31mE   AssertionError: test duration over 2x last duration\u001b[0m\n",
      "\u001b[1m\u001b[31m    assert 0.378446 <= (0.014618 * 2)\u001b[0m\n",
      "=========================== short test summary info ============================\n",
      "ERROR example_4/test_slower.py::test_slow_stuff[3] - AssertionError: test dur...\n",
      "\u001b[31m\u001b[32m5 passed\u001b[0m, \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 2.27s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Running again the tests will cause some tests to fail if their execution time\n",
    "# is more than double of the last execution.\n",
    "\n",
    "!pytest -q --tb=line {EXAMPLE_4}/test_slower.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 2.56s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The last example reads and writes to the cache for every test.\n",
    "# We can refactor it to read from cache once per session, as follows\n",
    "\n",
    "!pytest -q --cache-clear {EXAMPLE_4}/test_slower_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 2.22s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -q --tb=no {EXAMPLE_4}/test_slower_2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can test functions that write to stdout and stderr with the 'capsys' fixture\n",
    "\n",
    "!pytest -q --tb=no {EXAMPLE_5}/test_capsys.py::test_greeting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -q --tb=no {EXAMPLE_5}/test_capsys.py::test_yikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "always print this\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can print some line without using the '-s' argument with 'capsys.disabled()'\n",
    "\n",
    "!pytest -q {EXAMPLE_5}/test_capsys.py::test_capsys_disabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "always print this\n",
      "normal print, usually captured\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# With '-s' enabled, there are other lines that get printed\n",
    "\n",
    "!pytest -q -s {EXAMPLE_5}/test_capsys.py::test_capsys_disabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "example_6/test_cheese.py::test_def_prefs_change_home\n",
      "  /home/william/github/will-i-amv-books/python-testing-pytest/chapter04/example_6/test_cheese.py:13: PytestWarning: Value of environment variable HOME type should be str, but got local('/tmp/pytest-of-william/pytest-121/test_def_prefs_change_home0/home') (type: LocalPath); converted to str implicitly\n",
      "    monkeypatch.setenv('HOME', tmpdir.mkdir('home'))\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m4 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can \"monkey patch\" classes or modules with the 'monkeypatch' builtin fixture, as follows\n",
    "\n",
    "!pytest -q -s {EXAMPLE_6}/test_cheese.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0 -- /home/william/github/will-i-amv-books/python-testing-pytest/env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/chapter04\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "example_7/unnecessary_math.py::unnecessary_math \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 33%]\u001b[0m\n",
      "example_7/unnecessary_math.py::unnecessary_math.divide \u001b[32mPASSED\u001b[0m\u001b[32m            [ 66%]\u001b[0m\n",
      "example_7/unnecessary_math.py::unnecessary_math.multiply \u001b[32mPASSED\u001b[0m\u001b[32m          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Consider the following code with docstrings. We can test the code snippets in them\n",
    "# without adding import-related boilerplate with the 'doctest_namespace' fixture, as follows\n",
    "\n",
    "!pytest -v --doctest-modules --tb=short {EXAMPLE_7}/unnecessary_math.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/william/github/will-i-amv-books/python-testing-pytest/chapter04\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "example_1/test_warnings.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We can use the 'recwarn' fixture to print warnings if assert statements fail, \n",
    "# instead of forcing test failings as usual\n",
    "\n",
    "!pytest {EXAMPLE_1}/test_warnings.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "597ed2a883dd92248704c39695e4adb59d169bf662e25147d30ca9618a6f5c2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
